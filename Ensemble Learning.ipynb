{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66c54a1",
   "metadata": {},
   "source": [
    "A group of predictors (classifier or regressors) is called an ensemble. Random Forest Classifiers makes predictions by taking the class that gets the most votes from the predictions of several Decision Tree Classifiers. Usually Ensemble methods are used at the end of a project to combine a few good predictors into a better predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d11917",
   "metadata": {},
   "source": [
    "# Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f601a0",
   "metadata": {},
   "source": [
    "If we have several classifiers such as Logistic Regression, Support Vector Machine, Random Forest and they all have an accuracy of 80% we can improve this accuracy by aggregating the predictions of each classifier and predict the class that gets the most votes. This classifier which is called a hard voting classifier has a higher accuracy than the best classifier in the ensemble.\n",
    "If we have 1000 classifiers each with an accuracy of 51% then using a majority class vote we can get an accuracy up to 80% accuracy. Since this is a binomial problem an assumption is the classifiers are independently making predictions. This is most likely not the case because they are all being trained on the same data and are likely to make the same errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1089d",
   "metadata": {},
   "source": [
    "## Training a Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502d76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ac48718",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = make_moons()[0]\n",
    "y = make_moons()[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "066c31be",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "931bae90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "        estimators = [('lr', log_clf), ('rf', rf_clf), ('svc', svm_clf)],\n",
    "        voting = 'hard'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "990d214e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.6\n",
      "RandomForestClassifier 0.56\n",
      "SVC 0.64\n",
      "VotingClassifier 0.68\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rf_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(clf.__class__.__name__, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020fd66",
   "metadata": {},
   "source": [
    "The voting classifier outperforms the best model slightly. If all the classifier are able to predict probabilities we can use soft voting which gives more weight to the class with the highest probability. So it predicts the class with the highest average probability. In the code above SVC does not predict probabilities by default, so we can set the probability hyperparameter to True although this will reduce training time. In general soft voting performs better than hard voting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94308e43",
   "metadata": {},
   "source": [
    "# Bagging & Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9414c5",
   "metadata": {},
   "source": [
    "Using different algorithms for a voting classifier can improve performance because each algorithms makes predictions independently and so diversity is general good for ensemble methods. Another approach is to use one algorithm for all the predictors but to train it on different random subsets of the training data. Bagging (boostrap aggregating) uses sampling with replacement where as pasting uses sampling without replacement. With bagging a single training instance can be sampled svera; times for the same predictor and also several times across different predictors. But with pasting since it is done without replacement a training instance can only be sampled several times across different predictors. Once all predictors are trained predictions are made through the statistical mode for classification and the average for regression. Each individual predictor has a higher bias than if it were trained on the original training dataset but aggregation reduces bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b7756",
   "metadata": {},
   "source": [
    "## Implementation in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "090289aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4234192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=10,\n",
       "                  n_estimators=750, n_jobs=-1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 750, max_samples = 10, bootstrap = True, n_jobs = -1)\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8099f081",
   "metadata": {},
   "source": [
    "Here we are using a Decision Tree Classifier (500 of them) each trained on 20 random instances. The hyperparameter bootstrap = True allows us to use the bagging method and n_jobs = -1 means we use all CPU cores available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d8c49f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b2b1d",
   "metadata": {},
   "source": [
    "# Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbfd05",
   "metadata": {},
   "source": [
    "With bagging some instances may not be sampled. On average 63% of the training instances are sampled for each predictor. Since the predictor does not see these out-of-bag instances they can be used to evaluate the ensemble predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70dc97c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), max_samples=0.2,\n",
       "                  n_estimators=500, n_jobs=-1, oob_score=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500, max_samples = 0.2, \n",
    "                            bootstrap=True, n_jobs=-1, oob_score = True) # oob_score - out-bag-score set to true\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca16622d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db7869fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "y_proba = bag_clf.predict_proba(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776eeb0",
   "metadata": {},
   "source": [
    "So we can use the oob instances to evaluate the classifier to get a rough idea of how it will perform on the test set. This means we don't neccessarily need a validation set. Since the Decision Tree Classifier has a predict_proba() method the decision function returns the class probabilities for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9eec0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44691358, 0.55308642],\n",
       "       [0.36138614, 0.63861386],\n",
       "       [0.57178841, 0.42821159],\n",
       "       [0.39805825, 0.60194175],\n",
       "       [0.19211823, 0.80788177],\n",
       "       [0.56009615, 0.43990385],\n",
       "       [0.70263789, 0.29736211],\n",
       "       [0.5207824 , 0.4792176 ],\n",
       "       [0.15061728, 0.84938272],\n",
       "       [0.59158416, 0.40841584]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663a4a7",
   "metadata": {},
   "source": [
    "We can also sample features. This can be done in Scikit-Learn through the hyperparameters max_features and boostrap_features which work in the same way as max_samples and boostrap hyperparamter.\n",
    "\n",
    "\n",
    "## Random Patches & Subspaces\n",
    "\n",
    "Using all instances but sampling the features (max_samples = 1.0, bootstrap = False, max_features < 1.0, bootstrap_features = True) is called the Random Subspaces method. Sampling both features and instances is called the Random Patches method which can be useful for high dimensional data such as images. Sampling both features and instances increases bias slightly but reduces variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8bab13",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e734e7",
   "metadata": {},
   "source": [
    "The Random Forest algorithm is an ensemble of Decision Trees. It is trained via bagging with a max_samples hyperparameter set to the size of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad07fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ab25d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_leaf_nodes=16, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "274d600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca617a57",
   "metadata": {},
   "source": [
    "RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters of a BaggingClassifier to control the ensemble itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1059aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16), \n",
    "                            n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1) # This is equivalent to the RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cbcbce",
   "metadata": {},
   "source": [
    "The RandomForestClassifier introduces extra randomness as it searches for the best feature within a random subset of features instead of searching for the best feature when splitting a node. This increases bias but reduces variance.\n",
    "This randomness can be increased and such trees are called Extremely Randomized Trees. This increases bias while reducing bias but also reduces training time for extra trees. In Scikit-Learn Extremely Randomised Tree classifier can be created \n",
    "through ExtraTreesClassifier class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e403bf",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da2759a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38a48785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rf_clf.fit(load_iris().data, load_iris().target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c65b56f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.11158086355898864\n",
      "sepal width (cm) 0.025372528128335813\n",
      "petal length (cm) 0.42937722139604395\n",
      "petal width (cm) 0.4336693869166316\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(load_iris().feature_names, rf_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca15516",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21be1b",
   "metadata": {},
   "source": [
    "Boosting is an ensemble method which combines several weak learners into a strong learner. The predictors are trained sequentially, each trying to correct its predecessor. The two most common boosting methods are AdaBoost and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8037e4",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71abca1",
   "metadata": {},
   "source": [
    "In AdaBoost each predictor corrects it predecessor by increasing the relative weight of misclassified training instances. It then uses the updated weights and makes new predictions and then these weights are updated by the next predictor. This technique has many similarities with Gradient Descent. In Gradient Descent the parameters of a single predictor are updated to improve the model by minimising the cost function but in AdaBoost predictors are added to the ensemble gradually improving perfomance. After all the predictors are trained the ensemble makes predictions in a similar way to bagging but the predictors have different weights depending on their accuracy on the weighted training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce2f6d",
   "metadata": {},
   "source": [
    "The weight of each instance is initialised to $\\frac{1}{m}$ where m is the sample size. The weighted error of each predictor on the training set is\n",
    "\n",
    "$\\Large r_{j} = \\frac{\\sum_{i = 1}^{m} w^{(i)} \\, \\, \\, \\, \\hat{y}^{(i)}_{j} \\neq y^{(i)}}{\\sum_{i = 1}^{m} w^{(i)}}$\n",
    "\n",
    "\n",
    "The predictor's weight is computed using the weighted error\n",
    "\n",
    "$\\Large \\alpha_j = \\eta \\log \\frac{1 - r_j}{r_j}$\n",
    "\n",
    "If $r_j > 0.5 $ then $\\alpha_j < 0$ and if $r_j < 0.5$ and $\\alpha_j > 0$. If the error is close to 50% i.e. the predictor is randomly guessing the weight of the predictor is close to zero. If the error is greater then 50% i.e. the predictor is worse than random guessing then the weight is negative. If the error is less than 50% then the weight increases and is positive.\n",
    "\n",
    "The weight update rule\n",
    "\n",
    "$ \\Large w^{(i)} \\rightarrow w^{(i)} \\, \\, \\, \\, \\, \\, \\, \\, \\hat{y}^{(i)}_{j} = y^{(i)} \\\\$\n",
    "$ \\Large w^{(i)} \\rightarrow w^{(i)}\\exp\\left(\\alpha_j \\right) \\qquad \\hat{y}^{(i)}_{j} \\neq y^{(i)}$\n",
    "\n",
    "The weights are normalised by dividing by $\\sum_{i = 1}^{m} w^{(i)}$.\n",
    "\n",
    "The algorithm stops when the desired number of predictors are reached or when a perfect predictor is found.\n",
    "\n",
    "AdaBoost computes the predictions of all the predictors and weighs them using the predictors weights $\\alpha_j$. The predicted class is the one that receives the majority of weighted votes.\n",
    "\n",
    "$\\Large \\hat{y}(x) = argmax \\sum_{j = 1 }^{N} \\alpha_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659c885",
   "metadata": {},
   "source": [
    "## Implementation in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c439dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67d91b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=500)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1), n_estimators = 500, \n",
    "                             algorithm = \"SAMME.R\", learning_rate = 0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b452445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ada_pred = ada_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb4e3b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_ada_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73783843",
   "metadata": {},
   "source": [
    "Scikit-Learn uses a multiclass version of AdaBoost called SAMME. If the predictors can estimate class probabilities we can use a variant of SAMME called SAMME.R. This uses class probabilities and performs better than hard prediction.\n",
    "The AdaBoostClassifier above uses 200 DecisionTreeClassifiers with max_depth = 1. Some solutions when AdaBoost ensemble are overfitting the data is to limit the number of estimators or regularise the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589e55a1",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce7348",
   "metadata": {},
   "source": [
    "Gradient Boosting works similarly to AdaBoost. However, instead of adapting weights at each iteration Gradient Boosting fits the new predictors to the residual errors made by the previous predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe0247",
   "metadata": {},
   "source": [
    "## Example of Gradient Boosting in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d121762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6281844d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = 6*np.random.rand(200, 1) - 3\n",
    "y = X**2 + X + 5 + np.random.randn(200, 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel('X', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14, rotation=360, labelpad = 20)\n",
    "plt.scatter(X, y, c='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b8d73888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg1.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ba6bda7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y.ravel() - tree_reg1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3338c62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ea3393fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = y2.ravel() - tree_reg2.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "113b9116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8a23457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.8175926])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array([[2]])\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3)) # make predictions by adding predictions\n",
    "y_pred                                                                          # from each predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451604af",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor in Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a979ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fc40fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth = 2, n_estimators = 3, learning_rate = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321e6d4",
   "metadata": {},
   "source": [
    "Scikit-Learn's GradientBoosting class has hyperparameters to control the growth of the trees aswell as hyperparameters that control the ensemble itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4e863259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.8, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ab1819b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.93318127])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f933d6",
   "metadata": {},
   "source": [
    "Using a high learning rate with a low number of estimators can lead to underfitting while using a low learning rate with a high number of trees can lead to overfitting and so it is important to find the optimal number of estimators for a given learning rate. One way we can do this is by using early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9c1da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d92250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b4a2e",
   "metadata": {},
   "source": [
    "## Optimising Number of Trees Using Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d938441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=150)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(max_depth = 2, n_estimators=150)\n",
    "gbr.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26e94df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbr.staged_predict(X_val)]\n",
    "best_n_estimators = np.argmin(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c243f",
   "metadata": {},
   "source": [
    "The stage_predict() method returns the predictions made by the ensemble at each stage (at one tree, two trees, three trees etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3b9fc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=67)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_best_estimators = GradientBoostingRegressor(max_depth = 2, n_estimators = best_n_estimators)\n",
    "gbr_best_estimators.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc75eb5",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e8816c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e017d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63157b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4171475157781153"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = xgb_reg.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a0ecd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:2.31967\n",
      "[1]\tvalidation_0-rmse:1.81341\n",
      "[2]\tvalidation_0-rmse:1.51450\n",
      "[3]\tvalidation_0-rmse:1.32305\n",
      "[4]\tvalidation_0-rmse:1.26265\n",
      "[5]\tvalidation_0-rmse:1.19832\n",
      "[6]\tvalidation_0-rmse:1.17185\n",
      "[7]\tvalidation_0-rmse:1.16706\n",
      "[8]\tvalidation_0-rmse:1.17880\n"
     ]
    }
   ],
   "source": [
    "xgb_reg.fit(X_train, y_train, eval_set = [(X_val, y_val)])\n",
    "xgb_reg.set_params(early_stopping_rounds=2) # xgboost incorporates early stopping automatically\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
